@inproceedings{phutane2025ableistintersectionaldisabilitybias,
      title={ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios}, 
      author={Mahika Phutane* and Hayoung Jung* and Matthew Kim and Tanushree Mitra and Aditya Vashistha},
      year={2025},
      booktitle={Submission},
      abbr={Preprint},
      eprint={2510.10998},
      selected={true},
      category={preprint},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      code={https://github.com/hayoungjungg/ABLEIST},
      model={https://huggingface.co/hayoungjung/llama3.1-8b-adapter-ABLEist-detection},
      url={https://arxiv.org/abs/2510.10998}, 
      abstract = {Large language models (LLMs) are increasingly under scrutiny for perpetuating identity-based discrimination in high-stakes domains such as hiring, particularly against people with disabilities (PwD). However, existing research remains largely Western-centric, overlooking how intersecting forms of marginalization--such as gender and caste--shape experiences of PwD in the Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring scenarios spanning diverse disability, gender, nationality, and caste profiles. To capture subtle intersectional harms and biases, we introduce ABLEIST (Ableism, Inspiration, Superhumanization, and Tokenism), a set of five ableism-specific and three intersectional harm metrics grounded in disability studies literature. Our results reveal significant increases in ABLEIST harms towards disabled candidates--harms that many state-of-the-art models failed to detect. These harms were further amplified by sharp increases in intersectional harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates, highlighting critical blind spots in current safety tools and the need for intersectional safety evaluations of frontier models in high-stakes domains like hiring.}
}

@inproceedings{kaur2025whosaskingsimulatingrolebased,
      title={Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation}, 
      author={Navreet Kaur and Hayoung Jung* and Hoda Ayad* and Shravika Mittal and Munmun De Choudhury and Tanushree Mitra},
      year={2025},
      booktitle={Submission},
      abbr={Preprint},
      eprint={2510.16829},
      category={preprint},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.16829}, 
      abstract={Language model users often embed personal and social context in their questions. The asker's role -- implicit in how the question is framed -- creates specific needs for an appropriate response. However, most evaluations, while capturing the model's capability to respond, often ignore who is asking. This gap is especially critical in stigmatized domains such as opioid use disorder (OUD), where accounting for users' contexts is essential to provide accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for User-centric Question Simulation), a framework for simulating role-based questions. Drawing on role theory and posts from an online OUD recovery community (r/OpiatesRecovery), we first build a taxonomy of asker roles -- patients, caregivers, practitioners. Next, we use it to simulate 15,321 questions that embed each role's goals, behaviors, and experiences. Our evaluations show that these questions are both highly believable and comparable to real-world data. When used to evaluate five LLMs, for the same question but differing roles, we find systematic differences: vulnerable roles, such as patients and caregivers, elicit more supportive responses (+17%) and reduced knowledge content (-19%) in comparison to practitioners. Our work demonstrates how implicitly signaling a user's role shapes model responses, and provides a methodology for role-informed evaluation of conversational AI.}
}

@inproceedings{jung2025mythtriagescalabledetectionopioid,
      title={{M}yth{T}riage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform}, 
      author={Hayoung Jung and Shravika Mittal and Ananya Aatreya and Navreet Kaur and Munmun De Choudhury and Tanushree Mitra},
      year={2025},
      booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
      abbr={EMNLP},
      eprint={2506.00308},
      selected={true},
      category={published},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      data={https://github.com/hayoungjungg/MythTriage},
      code={https://github.com/hayoungjungg/MythTriage/tree/main/code},
      model={https://huggingface.co/SocialCompUW/youtube-opioid-myth-detect-M1},
      poster={EMNLP_2025_main-349_poster.pdf},
      slides={EMNLP_2025_main-349_slides.pdf},
      url={https://arxiv.org/abs/2506.00308},
      abstract = {Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)--a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation.}
}

@inproceedings{Jung_Juneja_Mitra_2025,
      title={Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation Between the United States and South Africa}, 
      author={Hayoung Jung and Prerna Juneja and Tanushree Mitra},
      year={2025},
      booktitle={Proceedings of the International AAAI Conference on Web and Social Media (ICWSM)},
      abbr={ICWSM},
      DOI={10.1609/icwsm.v19i1.35854},
      url={https://ojs.aaai.org/index.php/ICWSM/article/view/35854},
      category={published},
      volume={19},  
      number={1},
      pages={935-964},
      data={https://github.com/social-comp/YouTubeAuditGeolocation-data},
      model={https://huggingface.co/SocialCompUW/youtube-covid-misinfo-detect},
      slides={youtube-geolocation-slides.pdf},
      abstract = {Despite being an integral tool for finding health-related information online, YouTube has faced criticism for disseminating COVID-19 misinformation globally to its users. Yet, prior audit studies have predominantly investigated YouTube within the Global North contexts, often overlooking the Global South. To address this gap, we conducted a comprehensive 10-day geolocation-based audit on YouTube to compare the prevalence of COVID-19 misinformation in search results between the United States (US) and South Africa (SA), the countries heavily affected by the pandemic in the Global North and the Global South, respectively. For each country, we selected 3 geolocations and placed sock-puppets, or bots emulating "real" users, that collected search results for 48 search queries sorted by 4 search filters for 10 days, yielding a dataset of 915K results. We found that 31.55% of the top-10 search results contained COVID-19 misinformation. Among the top-10 search results, bots in SA faced significantly more misinformative search results than their US counterparts. Overall, our study highlights the contrasting algorithmic behaviors of YouTube search between two countries, underscoring the need for the platform to regulate algorithmic behavior consistently across different regions of the Globe.}
}

@inproceedings{mittal2025oudmythsredditllm,
      title={Online Myths on Opioid Use Disorder: A Comparison of Reddit and Large Language Model}, 
      author={Shravika Mittal and Hayoung Jung and Mai ElSherief and Tanushree Mitra and Munmun De Choudhury},
      year={2025},
      booktitle={Proceedings of the International AAAI Conference on Web and Social Media (ICWSM)},
      abbr={ICWSM},
      url={https://ojs.aaai.org/index.php/ICWSM/article/view/35870},
      category={published},
      volume={19}, 
      number={1},
      pages={1224-1245},
      DOI={10.1609/icwsm.v19i1.35870},
      code={https://github.com/anonymous-user-25/OUD-myths},
      slides={reddit-llm-oud.pdf},
      abstract = {Online communities on Reddit are a popular choice among people with opioid use disorder (OUD) to seek information on drug use, withdrawal symptoms, and recovery. LLMpowered chatbots (e.g., ChatGPT) are widely being adopted as question-answer systems for health-related queries. However, such online health information seeking could potentially be hindered by myths and misinformation on OUD, misleading or causing genuine harm to people with OUD. In this work, we examine the prevalence of 5 OUD-related myths, on treatment models and patient characteristics, within human-(taken from Reddit) and LLM-generated responses to queries on OUD. We further explore the framing strategies used within responses (both human- and LLM-generated) promoting and countering the myths. We found that all 5 myths were more widespread within human-generated responses. In addition, myth-promoting responses adopted trustworthy and authoritative framings, compared to knowledge-imparting linguistic cues within those countering the myths. Our work offers recommendations to reduce online OUD misinformation.}
}


@inproceedings{park-etal-2024-valuescope,
    title = "{V}alue{S}cope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions",
    author = "Park*, Chan Young  and
      Li*, Shuyue Stella  and
      Jung*, Hayoung  and
      Volkova, Svitlana  and
      Mitra, Tanu  and
      Jurgens, David  and
      Tsvetkov, Yulia",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    abbr={EMNLP},
    category={published},
    code={https://github.com/stellalisy/valueScope},
    poster={valuescope-poster.pdf},
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.972",
    pages = "16659--16695",
    abstract = "This study introduces ValueScope, a framework leveraging language models to quantify social norms and values within online communities, grounded in social science perspectives on normative structures. We employ ValueScope to dissect and analyze linguistic and stylistic expressions across 13 Reddit communities categorized under gender, politics, science, and finance. Our analysis provides a quantitative foundation confirming that even closely related communities exhibit remarkably diverse norms. This diversity supports existing theories and adds a new dimension to understanding community interactions. ValueScope not only delineates differences in social norms but also effectively tracks their evolution and the influence of significant external events like the U.S. presidential elections and the emergence of new sub-communities. The framework thus highlights the pivotal role of social norms in shaping online interactions, presenting a substantial advance in both the theory and application of social norm studies in digital spaces.",
}

@inproceedings{dammu-etal-2024-uncultured,
    title = "{``}They are uncultured{''}: Unveiling Covert Harms and Social Threats in {LLM} Generated Conversations",
    author = "Dammu*, Preetam Prabhu Srikar  and
      Jung*, Hayoung  and
      Singh, Anjali  and
      Choudhury, Monojit  and
      Mitra, Tanu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    selected={true},
    abbr={EMNLP}, 
    category={published},
    model={https://huggingface.co/SocialCompUW/CHAST},
    slides={llm-culture-slides.pdf},
    award={-- Nominated for the Best Paper Award},
    uw_news={https://www.washington.edu/news/2024/11/20/ai-chatbots-chatgpt-hiring-bias-caste-race/},
    mit_tech={https://www.technologyreview.com/2025/10/01/1124621/openai-india-caste-bias/},
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1134",
    pages = "20339--20369",
    abstract = "Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate {``}harm{''} as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.",
}
